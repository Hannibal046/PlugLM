# PlugLM: Language model with Plug-in Knowledge Memory

This reposotory contains the code and data for this paper [PlugLM: Language model with Plug-in Knowledge Memory](https://openreview.net/forum?id=Plr5l7r0jY6). 

For the first time, we challenge the current implicit knowledge encoding mechanism for PLMs, which have two fundamental drawbacks: 

- The knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. 
- It lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. 

Based on the discovery that the Feed-Forward Network in PLM has the ability to store various types of knowledge during pre-training and is fundamentally a key-value memory network, we present **PlugLM**. This is the first model to separate knowledge storage from model parameters by utilizing an editable and scalable key-value memory, allowing for more adaptable and understandable knowledge encoding in PLMs.

We believe this architectural design would pave a new direction for future research on language model pre-training, especially for LLM.


![model](assets/model.svg)

## Setup
Our pretraining code is based on the [NVIDIA/BERT](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT). 

First install the requirements listed there including:
- [PyTorch](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT/scripts/docker)
- [lddl](https://github.com/NVIDIA/lddl) is a utility library that minimizes the friction during dataset retrieval, preprocessing and loading for the language models in NVIDIA Deep Learning Examples.
- [APEX](https://github.com/NVIDIA/apex) is a PyTorch extension with NVIDIA-maintained utilities to streamline mixed precision and distributed training, whereas AMP is an abbreviation used for automatic mixed precision training.
- [LAMB](https://arxiv.org/abs/1904.00962v1) stands for Layerwise Adaptive Moments based optimizer, is a large batch optimization technique that helps accelerate training of deep neural networks using large minibatches.

Then install the following packages:
```bash
pip install transformers tqdm numpy datasets
``` 

## Dataset
For pre-training data and pre-processing, we use [this scripts](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/scripts/data_download.sh) to download Wikipedia.

For S2ORC pre-training and fine-tuning data, we send application form and download from [here](https://github.com/allenai/s2orc).

For PubMed pre-training data, we download from [here](https://github.com/naver/biobert-pretrained), and Biomedical downstream dataset [here](https://github.com/dmis-lab/biobert).

For other datasets, we use [datasets](https://github.com/huggingface/datasets) to download.

## Pre-training
The pre-training code (on 8 A100):
```bash
cd src
python -m torch.distributed.launch --nproc_per_node 8 --master_port 12344 --use_env \
    run_pretraining.py \
        --data_dir ../data/phase2 \
        --output_dir ../results/retrieval_bert_token_level_6_12_mlm\
        --tokenized_document_path ../data/tokenized_wiki.npy \
        --pretrained_bert_path ../pretrained_model/bert_base_uncased/pytorch_model.bin \
        --total_train_batch_size_per_device 8192 \
        --gradient_accumulation_steps 256 \
        --max_train_steps 8000 \
        --refreshing_steps 200 \
        --ckpt_steps 500 \
        --init_from_bert \
        --learning_rate=6e-3 \
        --refreshing_batch_size 4096 \
        --knowledge_attention_type multi_head \



```