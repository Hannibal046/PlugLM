# PlugLM: Language model with Plug-in Knowledge Memory

This reposotory contains the code and data for this paper [PlugLM: Language model with Plug-in Knowledge Memory](https://openreview.net/forum?id=Plr5l7r0jY6). 

<!-- PLMs have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training.

However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks:


In this paper, we introduce **PlugLM**, a pre-training model with differentiable plug-in memory(DPM). 

The key intuition is to decouple the knowledge storage from model parameters(i.e. Feed-Forward Network, where the knowledge is stored in Transformer) with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM.  -->

For the first time, we challenge the current implicit knowledge encoding mechanism for PLMs, which have two fundamental drawbacks: 

- The knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. 
- It lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. 

Inspired by the findings that Feed-forward Network in PLM stores all kinds of knowledge and is essentially a key-value memory network, we propose to decouple knowledge storage from model parameters with an editable and scalable key-value memory, which makes the knowledge encoding of PLMs more flexible and interpretable. 

We believe this architectural design would pave a new direction for future research on PLM, especially for LLM.


![model](assets/model.svg)