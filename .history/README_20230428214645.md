# PlugLM: Language model with Plug-in Knowledge Memory

This reposotory contains the code and data for this paper [PlugLM: Language model with Plug-in Knowledge Memory](https://openreview.net/forum?id=Plr5l7r0jY6). 

<!-- PLMs have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training.

However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks:


In this paper, we introduce **PlugLM**, a pre-training model with differentiable plug-in memory(DPM). 

The key intuition is to decouple the knowledge storage from model parameters(i.e. Feed-Forward Network, where the knowledge is stored in Transformer) with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM.  -->

For the first time, we challenge the current implicit knowledge encoding mechanism for PLMs, which have two fundamental drawbacks: 

- The knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. 
- It lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. 

Based on the discovery that the Feed-Forward Network in PLM has the ability to store various types of knowledge during pre-training and is fundamentally a key-value memory network, we present **PlugLM**. This is the first model to separate knowledge storage from model parameters by utilizing an editable and scalable key-value memory, allowing for more adaptable and understandable knowledge encoding in PLMs.

We believe this architectural design would pave a new direction for future research on language model pre-training, especially for LLM.


![model](assets/model.svg)

## Setup
1. install 
```bash

```