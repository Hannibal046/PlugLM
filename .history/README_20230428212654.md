# PlugLM: Language model with Plug-in Knowledge Memory

This reposotory contains the code and data for this paper [PlugLM: Language model with Plug-in Knowledge Memory](https://openreview.net/forum?id=Plr5l7r0jY6). 

Pre-trained language models(PLM) have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training.

However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. In this paper, we introduce \model, a pre-training model with differentiable plug-in memory~(DPM). The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the \memory. 
To justify this design choice, we conduct evaluations in three settings including:~(1) domain adaptation. \model obtains 3.95 F1 improvements across four domains on average without any in-domain pre-training. (2) knowledge update. \model could absorb new knowledge in a training-free way after pre-training is done. (3) in-task knowledge learning. \model could be further improved by incorporating training samples into \memory with knowledge prompting.

![model](assets/model.svg)