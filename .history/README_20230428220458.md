# PlugLM: Language model with Plug-in Knowledge Memory

This reposotory contains the code and data for this paper [PlugLM: Language model with Plug-in Knowledge Memory](https://openreview.net/forum?id=Plr5l7r0jY6). 

<!-- PLMs have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training.

However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks:


In this paper, we introduce **PlugLM**, a pre-training model with differentiable plug-in memory(DPM). 

The key intuition is to decouple the knowledge storage from model parameters(i.e. Feed-Forward Network, where the knowledge is stored in Transformer) with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM.  -->

For the first time, we challenge the current implicit knowledge encoding mechanism for PLMs, which have two fundamental drawbacks: 

- The knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. 
- It lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. 

Based on the discovery that the Feed-Forward Network in PLM has the ability to store various types of knowledge during pre-training and is fundamentally a key-value memory network, we present **PlugLM**. This is the first model to separate knowledge storage from model parameters by utilizing an editable and scalable key-value memory, allowing for more adaptable and understandable knowledge encoding in PLMs.

We believe this architectural design would pave a new direction for future research on language model pre-training, especially for LLM.


![model](assets/model.svg)

## Setup
Our pretraining code is based on the [NVIDIA/BERT](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT). 

First install the requirements listed there including:
- [PyTorch](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT/scripts/docker)
- [lddl](https://github.com/NVIDIA/lddl) is a utility library that minimizes the friction during dataset retrieval, preprocessing and loading for the language models in NVIDIA Deep Learning Examples.
- [APEX](https://github.com/NVIDIA/apex) is a PyTorch extension with NVIDIA-maintained utilities to streamline mixed precision and distributed training, whereas AMP is an abbreviation used for automatic mixed precision training.
- [LAMB](https://arxiv.org/abs/1904.00962v1) stands for Layerwise Adaptive Moments based optimizer, is a large batch optimization technique that helps accelerate training of deep neural networks using large minibatches.

Then install the following packages:
```bash
pip install transformers tqdm numpy datasets
``` 

## Dataset
For pretraining data and pre-processing, we use [this scripts](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/scripts/data_download.sh) to download Wikipedia.

For S2ORC data, we send application form and download from [here](https://github.com/allenai/s2orc).

For other datasets, we use `